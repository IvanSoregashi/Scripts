{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разработать простейший TCP echo сервер.\n",
    "\n",
    "Требования\n",
    "\n",
    "Запускается на IP адресе 0.0.0.0 и TCP порту 2222 \n",
    "Получает сообщения длинной не более 1024 байт и отправляет обратно клиенту\n",
    "Закрывает соединение при получении сообщения с текстом close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind(('0.0.0.0', 2222))\n",
    "s.listen(1)\n",
    "conn, addr = s.accept()\n",
    "while True:\n",
    "    data = conn.recv(1024)\n",
    "    if not data or data == 'close':\n",
    "        conn.close()\n",
    "        break\n",
    "    else:\n",
    "        conn.send(data)\n",
    "        print(data)\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy.bmp\n",
      "mario.bmp\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     group_size[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mKB\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mKB\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mMB\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mMB\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mGB\u001b[39m\u001b[39m'\u001b[39m}[group_size[\u001b[39m1\u001b[39m]]\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39m[line[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(group)], sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m----------\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSummary: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(group_size)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "with open('files.txt', encoding='utf-8') as f:\n",
    "    files = [line.split() for line in f]\n",
    "    ext = set(line[0].split('.')[-1] for line in files)\n",
    "    for i in sorted(ext):\n",
    "        group = [line for line in files if line[0].endswith(i)]\n",
    "        sizes = {'B': 1, 'KB': 1024, 'MB': 1024*1024, 'GB': 1024*1024*1024}\n",
    "        group_size = [sum([int(line[1]) * sizes[line[2]] for line in group]), 'B']\n",
    "        while group_size[0]>1023:\n",
    "            group_size[0] = group_size[0]//1024 + (group_size[0]%1024 > 512)\n",
    "            group_size[1] = {'B':'KB','KB':'MB','MB':'GB'}[group_size[1]]\n",
    "        print(*[line[0] for line in sorted(group)], sep='\\n')\n",
    "        print('----------\\nSummary:', *group_size, )\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 11, 1111, 13, 544, 34]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_duplicates = lambda m: [n.append(i) is None and n[-1] for n in [[]] for i in m if len(n+[i]) == len(set(n)|{i})]\n",
    "\n",
    "remove_duplicates([1,1,1,1,1,1,11,1,1,1,1,1,1111,13,13,544,34])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from datetime import date as dt\n",
    "from calendar import month_name as mn, monthrange as mr\n",
    "\n",
    "def get_all_mondays(year):\n",
    "    fm = [d for d in range(dt(year, 1, 1).toordinal(), dt(year, 1, 7).toordinal()) if dt.fromordinal(d).weekday()==0]\n",
    "    return [dt.fromordinal(d) for d in range(*fm, dt(year+1, 1, 1).toordinal(), 7)]\n",
    "print(get_all_mondays(2021))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bridge of San Luis Rey,2005,4\n",
      "Godsend,2003,4\n",
      "The Big Wedding,2013,7\n",
      "New Year's Eve,2011,7\n",
      "The Bag Man,2014,9\n",
      "Little Fockers,2010,10\n",
      "Killing Season,2013,11\n",
      "Dirty Grandpa,2016,11\n",
      "Hide and Seek,2005,13\n",
      "Bloody Mama,1970,17\n",
      "Righteous Kill,2008,19\n",
      "Arthur and the Invisibles,2007,21\n",
      "Killer Elite,2011,25\n",
      "Heist,2015,26\n",
      "Analyze That,2002,27\n",
      "Red Lights,2012,29\n",
      "Grudge Match,2013,29\n",
      "Stanley & Iris,1990,29\n",
      "15 Minutes,2001,33\n",
      "Shark Tale,2004,35\n",
      "Great Expectations,1998,38\n",
      "Meet the Fockers,2004,38\n",
      "The Fan,1996,38\n",
      "Mary Shelley's Frankenstein,1994,39\n",
      "Born to Win,1971,40\n",
      "The Last Tycoon,1976,41\n",
      "Men of Honor,2000,41\n",
      "The Adventures of Rocky & Bullwinkle,2000,43\n",
      "Flawless,1999,43\n",
      "Last Vegas,2013,46\n",
      "Everybody's Fine,2009,46\n",
      "Rent,2005,46\n",
      "We're No Angels,1989,47\n",
      "City by the Sea,2002,48\n",
      "Stone,2010,50\n",
      "Being Flynn,2012,51\n",
      "What Just Happened?,2008,51\n",
      "The Good Shepherd,2006,54\n",
      "Falling in Love,1984,60\n",
      "Joy,2015,60\n",
      "The Intern,2015,61\n",
      "Jacknife,1989,64\n",
      "The Mission,1986,65\n",
      "Guilty by Suspicion,1991,65\n",
      "Night and the City,1992,67\n",
      "New York,1977,67\n",
      "Ronin,1998,68\n",
      "Mistress,1991,69\n",
      "Analyze This,1999,69\n",
      "Limitless,2011,70\n",
      "Backdraft,1991,71\n",
      "Machete,2010,72\n",
      "Cop Land,1997,72\n",
      "The Score,2001,73\n",
      "Sleepers,1996,74\n",
      "This Boy's Life,1993,75\n",
      "True Confessions,1981,75\n",
      "Cape Fear,1991,76\n",
      "Captain Shakespeare,2007,76\n",
      "Mad Dog and Glory,1993,78\n",
      "Angel Heart,1987,78\n",
      "Marvin's Room,1996,80\n",
      "The Untouchables,1987,80\n",
      "Casino,1995,80\n",
      "Meet the Parents,2000,84\n",
      "Wag the Dog,1997,85\n",
      "Heat,1995,86\n",
      "Greetings,1968,86\n",
      "Jackie Brown,1997,87\n",
      "Thunderheart,1992,87\n",
      "Bang the Drum Slowly,1973,88\n",
      "Awakenings,1990,88\n",
      "Once Upon a Time in America,1984,89\n",
      "The King of Comedy,1983,90\n",
      "Silver Linings Playbook,2012,92\n",
      "The Deer Hunter,1978,93\n",
      "A Bronx Tale,1993,96\n",
      "Midnight Run,1988,96\n",
      "Goodfellas,1990,96\n",
      "Raging Bull,1980,97\n",
      "Brazil,1985,98\n",
      "Mean Streets,1973,98\n",
      "Taxi Driver,1976,99\n",
      "Dear America: Letters Home From Vietnam,1987,100\n"
     ]
    }
   ],
   "source": [
    "import csv, sys\n",
    "\n",
    "with open('deniro.csv', 'r', encoding='utf-8') as f:\n",
    "    r = int(input()) - 1\n",
    "    s = sorted(list(csv.reader(f)), key=lambda x: int(x[r]) if x[r].isdigit() else x[r])\n",
    "    csv.writer(sys.stdout).writerows(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.csv', 'r', encoding='utf-8') as fin, open('domain_usage.csv', 'w', encoding='utf-8', newline='') as fout:\n",
    "    emails = list(zip(*__import__('csv').reader(fin)))[2][1:]\n",
    "    data = {}\n",
    "    for e in emails: data[e] = data.get((e:=e.split('@')[1]), 0) + 1\n",
    "    data = [('domain', 'count')] + list(sorted(data.items(), key=lambda x: (x[1], x[0])))\n",
    "    __import__('csv').writer(fout).writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тверской район: 480\n",
      "район Хамовники: 386\n",
      "Пресненский район: 349\n",
      "Басманный район: 343\n",
      "район Замоскворечье: 308\n",
      "Мещанский район: 228\n",
      "Таганский район: 212\n",
      "район Якиманка: 201\n",
      "район Арбат: 184\n",
      "Красносельский район: 142\n",
      "район Дорогомилово: 92\n",
      "Донской район: 51\n",
      "Даниловский район: 47\n",
      "район Беговой: 35\n",
      "район Марьина Роща: 18\n",
      "Южнопортовый район: 6\n",
      "Гагаринский район: 4\n",
      "Хорошёвский район: 4\n",
      "район Марьино: 4\n",
      "район Проспект Вернадского: 2\n",
      "район Лефортово: 1\n"
     ]
    }
   ],
   "source": [
    "with open('wifi.csv', encoding='utf-8') as f:\n",
    "    next(f)\n",
    "    data = {}\n",
    "    [data.setdefault(d, []).append(int(n)) for a, d, l, n in __import__('csv').reader(f, delimiter=';')]\n",
    "    data = sorted([(d, sum(n)) for d, n in data.items()], key=lambda x: (-x[1], x[0]))\n",
    "    print('\\n'.join([f'{d}: {n}' for d, n in data]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, datetime\n",
    "with open('name_log.csv', encoding='utf-8') as rf, open('new_name_log.csv', 'w', encoding='utf-8', newline='') as wf:\n",
    "    reader, writer = csv.reader(rf), csv.writer(wf)\n",
    "    writer.writerow(next(reader))\n",
    "    dt = lambda x: datetime.datetime.strptime(x[2], '%d/%m/%Y %H:%M')\n",
    "    data = {}\n",
    "    for n, e, t in sorted(reader, key=dt): data[e] = n, t\n",
    "    writer.writerows((n, e, t) for e, (n, t) in sorted(data.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,Title\n",
      "01,Ran So Hard the Sun Went Down\n",
      "02,Honky Tonk Heroes (Like Me)\n"
     ]
    }
   ],
   "source": [
    "def condense_csv(f, id_name='object'):\n",
    "    with open(f, encoding='utf-8') as rf, open('condensed.csv', 'w', encoding='utf-8', newline='') as wf:\n",
    "        reader = __import__('csv').reader(rf)\n",
    "        data = {}\n",
    "        [data.setdefault(o, {}).update({id_name: o, p:v}) for o, p, v  in reader]\n",
    "        writer =  __import__('csv').DictWriter(wf, fieldnames=list(next(iter(data.values())).keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data.values())\n",
    "\n",
    "text = '''01,Title,Ran So Hard the Sun Went Down\n",
    "02,Title,Honky Tonk Heroes (Like Me)'''\n",
    "\n",
    "with open('data.csv', 'w', encoding='utf-8') as file:\n",
    "    file.write(text)\n",
    "\n",
    "condense_csv('data.csv', id_name='ID')\n",
    "\n",
    "with open('condensed.csv', encoding='utf-8') as file:\n",
    "    print(file.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('student_counts.csv', encoding='utf-8') as rf: year, *classes = zip(*csv.reader(rf))\n",
    "classes = sorted(classes, key=lambda x: (int(x[0].split('-')[0]), x[0].split('-')[1]))\n",
    "\n",
    "with open('sorted_student_counts.csv', 'w', encoding='utf-8', newline='') as wf: csv.writer(wf).writerows(zip(year, *classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вареники: Дикси\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('prices.csv', encoding='utf-8') as rf:\n",
    "    #reader = csv.reader(rf, delimiter=';')\n",
    "    #header, *prices = reader\n",
    "    #shop, *prices = zip(*prices)\n",
    "    #print(*zip(shop, map(lambda x: min(map(int, x)), prices)))\n",
    "    reader = csv.DictReader(rf, delimiter=';')\n",
    "    m = [(n[1], *min(p, key=lambda x: int(x[1]))) for d in reader for n, *p in [d.items()]]\n",
    "    s, i, p = next(iter(sorted(m, key=lambda x: (int(x[2]), x[1], x[0]))))\n",
    "    print(f'{i}: {s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hello!\", 180, false, [1, 2, 3, 1, 2, 3], {\"key\": \"value\", \"newkey\": null}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "d = {str: lambda x: x+'!', bool: lambda x: not x, int: lambda x: x+1, list: lambda x: x*2, dict: lambda x: x|{\"newkey\": None}}\n",
    "process = lambda x: d[type(x)](x)\n",
    "#with open('data.json', encoding='utf-8') as rf, open('updated_data.json', 'w', encoding='utf-8') as wf:\n",
    "#    j = json.load(rf)\n",
    "#    json.dump([process(x) for x in j if x is not None], wf)\n",
    "print(json.dumps([process(x) for x in [\"Hello\", 179, True, None, [1, 2, 3], {\"key\": \"value\"}] if x is not None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "rel = {}\n",
    "with open('countries.json', encoding='utf-8') as f: counries = json.load(f)\n",
    "[rel.setdefault(r, []).append(c) for e in counries for c, r in [e.values()]]\n",
    "with open('religion.json', 'w', encoding='utf-8') as f: json.dump(rel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "p = {}\n",
    "with open('people.json', encoding='utf-8') as f: people = json.load(f)\n",
    "for i in people: p |= i\n",
    "for i in p: p[i] = None\n",
    "print(p.keys() == max(people, key=len).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json\n",
    "adr = {}\n",
    "with open('playgrounds.csv', encoding='utf-8') as f:\n",
    "    for o, a, d, r in csv.reader(f, delimiter=';'):\n",
    "        adr.setdefault(a, {}).setdefault(d, []).append(r)\n",
    "with open('addresses.json', 'w', encoding='utf-8') as f: json.dump(adr, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
